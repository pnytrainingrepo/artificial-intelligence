{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The        det        cat       \n",
      "cat        nsubj      sat       \n",
      "sat        ROOT       sat       \n",
      "on         prep       sat       \n",
      "the        det        mat       \n",
      "mat        pobj       on        \n",
      ".          punct      sat       \n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the SpaCy model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Process the text\n",
    "text = \"The cat sat on the mat.\"\n",
    "doc = nlp(text)\n",
    "\n",
    "# Print the dependency parse\n",
    "for token in doc:\n",
    "    print(f'{token.text:10} {token.dep_:10} {token.head.text:10}')\n",
    "\n",
    "# Output structure (word, dependency relation, head word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('programming', 0.1901046484708786), ('sat', 0.17440013587474823), ('are', 0.11519220471382141), ('pets', 0.10159842669963837), ('python', 0.08061393350362778), ('the', 0.04067763686180115), ('love', -0.02331056445837021), ('great', -0.029589535668492317), ('in', -0.03339873254299164), ('dogs', -0.06483341753482819)]\n"
     ]
    }
   ],
   "source": [
    "# 2. Semantics: Word Embeddings with Gensim\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "# Sample sentences\n",
    "sentences = [\n",
    "    \"The cat sat on the mat.\",\n",
    "    \"Dogs are great pets.\",\n",
    "    \"I love programming in Python.\"\n",
    "]\n",
    "\n",
    "# Tokenize the sentences\n",
    "tokenized_sentences = [simple_preprocess(sentence) for sentence in sentences]\n",
    "\n",
    "# Train a Word2Vec model\n",
    "model = Word2Vec(sentences=tokenized_sentences, vector_size=50, window=3, min_count=1, sg=1)\n",
    "\n",
    "# Find similar words\n",
    "similar_words = model.wv.most_similar('cat')\n",
    "print(similar_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment polarity: 0.5125\n",
      "Sentiment subjectivity: 0.75\n"
     ]
    }
   ],
   "source": [
    "# 3. Pragmatics: Sentiment Analysis with TextBlob\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Sample text\n",
    "text = \"I love this movie! It's fantastic.\"\n",
    "blob = TextBlob(text)\n",
    "\n",
    "# Get the sentiment\n",
    "sentiment = blob.sentiment\n",
    "print(f'Sentiment polarity: {sentiment.polarity}')\n",
    "print(f'Sentiment subjectivity: {sentiment.subjectivity}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "[E046] Can't retrieve unregistered extension attribute 'coref_clusters'. Did you forget to call the `set_extension` method?",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m doc \u001b[38;5;241m=\u001b[39m nlp(text)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Print coreferences\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cluster \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdoc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcoref_clusters\u001b[49m:\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCluster: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m[mention\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mmention\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mcluster\u001b[38;5;241m.\u001b[39mmentions]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Azam\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\spacy\\tokens\\underscore.py:48\u001b[0m, in \u001b[0;36mUnderscore.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extensions:\n\u001b[1;32m---> 48\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE046\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname))\n\u001b[0;32m     49\u001b[0m     default, method, getter, setter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extensions[name]\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m getter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mAttributeError\u001b[0m: [E046] Can't retrieve unregistered extension attribute 'coref_clusters'. Did you forget to call the `set_extension` method?"
     ]
    }
   ],
   "source": [
    "# 4. Discourse: Coreference Resolution with SpaCy\n",
    "\n",
    "import spacy\n",
    "import neuralcoref\n",
    "\n",
    "# Load the SpaCy model\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "# Add neuralcoref to the SpaCy pipeline\n",
    "neural_coref = neuralcoref.NeuralCoref(nlp.vocab)\n",
    "nlp.add_pipe(neural_coref, name='neural_coref', last=True)\n",
    "\n",
    "# Process the text\n",
    "text = \"John went to the store. He bought some apples.\"\n",
    "doc = nlp(text)\n",
    "\n",
    "# Print coreferences\n",
    "if doc._.has_coref:\n",
    "    for cluster in doc._.coref_clusters:\n",
    "        print(f\"Cluster: {[mention.text for mention in cluster.mentions]}\")\n",
    "else:\n",
    "    print(\"No coreferences found.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
